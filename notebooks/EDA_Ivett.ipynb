{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e781124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MLOps EDA Pipeline\n",
    "==================\n",
    "Exploratory Data Analysis for training data validation and quality checks.\n",
    "Produces artifacts for downstream pipeline stages.\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# IMPORTS AND CONFIGURATION\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Non-interactive backend to prevent memory leaks\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "try:\n",
    "    import missingno as msno\n",
    "    HAS_MISSINGNO = True\n",
    "except ImportError:\n",
    "    HAS_MISSINGNO = False\n",
    "\n",
    "# Configure warnings and pandas display\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.float_format', lambda x: \"%.3f\" % x)\n",
    "\n",
    "# Plotting defaults\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams.update({\"figure.figsize\": (10, 6), \"figure.dpi\": 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74d9744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# LOGGING SETUP\n",
    "# =============================================================================\n",
    "def setup_logger(name: str = \"eda_pipeline\", level: int = logging.INFO) -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Configure logger for the EDA pipeline.\n",
    "    \n",
    "    Args:\n",
    "        name: Logger name\n",
    "        level: Logging level\n",
    "        \n",
    "    Returns:\n",
    "        Configured logger instance\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    \n",
    "    # Console handler\n",
    "    if not logger.handlers:\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "logger = setup_logger()\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "class Config:\n",
    "    \"\"\"Configuration constants for the EDA pipeline.\"\"\"\n",
    "    ARTIFACTS_DIR = Path(\"./artifacts\")\n",
    "    PLOTS_DIR = ARTIFACTS_DIR / \"plots\"\n",
    "    DATA_PATH = ARTIFACTS_DIR / \"raw_data.csv\"\n",
    "    \n",
    "    # Date range (can be parameterized)\n",
    "    MAX_DATE = \"2024-03-31\"\n",
    "    MIN_DATE = \"2023-12-01\"\n",
    "    \n",
    "    # EDA parameters\n",
    "    MAX_HIST_COLS = 20  # Limit histograms to prevent memory issues\n",
    "    MAX_CAT_TOP = 10     # Top categories to display\n",
    "    \n",
    "    @classmethod\n",
    "    def setup_directories(cls):\n",
    "        \"\"\"Create necessary directories.\"\"\"\n",
    "        cls.ARTIFACTS_DIR.mkdir(exist_ok=True)\n",
    "        cls.PLOTS_DIR.mkdir(exist_ok=True)\n",
    "        logger.info(f\"Created artifacts directories: {cls.ARTIFACTS_DIR}, {cls.PLOTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "228adc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting                                            |0.00 [00:00,    ?entry/s]\n",
      "Fetching\n",
      "!\u001b[A\n",
      "  0% Checking cache in '/home/ivett/Documents/code/MLOPs/project/itu-sdse-projec\u001b[A\n",
      "                                                                                \u001b[A\n",
      "!\u001b[A\n",
      "  0% Checking cache in 'https://github.com/Jeppe-T-K/itu-sdse-project-data/files\u001b[A\n",
      "\u001b[33mWARNING\u001b[39m: Some of the cache files do not exist neither locally nor on remote. Missing cache files:\n",
      "md5: 05663184eaf8f324162e4d1bc14be387\n",
      "Fetching\n",
      "Building workspace index                             |8.00 [00:00, 2.27kentry/s]\n",
      "Comparing indexes                                    |9.00 [00:00, 1.41kentry/s]\n",
      "Applying changes                                      |0.00 [00:00,     ?file/s]\n",
      "Everything is up to date.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!dvc pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1640dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "def to_json_safe(obj):\n",
    "    \"\"\"\n",
    "    Convert numpy/pandas types to JSON-serializable Python types.\n",
    "    \n",
    "    Args:\n",
    "        obj: Object to convert\n",
    "        \n",
    "    Returns:\n",
    "        JSON-safe version of the object\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): to_json_safe(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [to_json_safe(v) for v in obj]\n",
    "    elif isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (pd.Timestamp, datetime.date, datetime.datetime)):\n",
    "        return obj.isoformat()\n",
    "    elif isinstance(obj, np.generic):\n",
    "        return obj.item()\n",
    "    elif isinstance(obj, (int, float, str, bool, type(None))):\n",
    "        return obj\n",
    "    else:\n",
    "        return str(obj)\n",
    "\n",
    "def save_plot(fig: plt.Figure, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Save matplotlib figure and properly close it to prevent memory leaks.\n",
    "    \n",
    "    Args:\n",
    "        fig: Matplotlib figure object\n",
    "        filename: Output filename\n",
    "    \"\"\"\n",
    "    try:\n",
    "        filepath = Config.PLOTS_DIR / filename\n",
    "        fig.savefig(filepath, bbox_inches='tight', dpi=120)\n",
    "        logger.debug(f\"Saved plot: {filename}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save plot {filename}: {e}\")\n",
    "    finally:\n",
    "        plt.close(fig)  # Critical for memory management\n",
    "\n",
    "def describe_numeric_col(x: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate descriptive statistics for a numeric column.\n",
    "    \n",
    "    Args:\n",
    "        x: Pandas Series (numeric)\n",
    "        \n",
    "    Returns:\n",
    "        Series with descriptive stats\n",
    "    \"\"\"\n",
    "    return pd.Series({\n",
    "        \"Count\": x.count(),\n",
    "        \"Missing\": x.isnull().sum(),\n",
    "        \"Mean\": x.mean(),\n",
    "        \"Std\": x.std(),\n",
    "        \"Min\": x.min(),\n",
    "        \"25%\": x.quantile(0.25),\n",
    "        \"50%\": x.quantile(0.50),\n",
    "        \"75%\": x.quantile(0.75),\n",
    "        \"Max\": x.max()\n",
    "    })\n",
    "\n",
    "def impute_missing_values(x: pd.Series, method: str = \"mean\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Impute missing values in a column.\n",
    "    \n",
    "    Args:\n",
    "        x: Pandas Series\n",
    "        method: Imputation method (\"mean\", \"median\", \"mode\")\n",
    "        \n",
    "    Returns:\n",
    "        Series with imputed values\n",
    "    \"\"\"\n",
    "    if x.dtype in ['float64', 'int64']:\n",
    "        if method == \"mean\":\n",
    "            return x.fillna(x.mean())\n",
    "        elif method == \"median\":\n",
    "            return x.fillna(x.median())\n",
    "    \n",
    "    # For categorical/other types, use mode\n",
    "    if not x.mode().empty:\n",
    "        return x.fillna(x.mode()[0])\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11e3fc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# =============================================================================\n",
    "def load_and_filter_data(\n",
    "    data_path: Path,\n",
    "    min_date: str,\n",
    "    max_date: str\n",
    ") -> Tuple[pd.DataFrame, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Load data and apply date filtering.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to the CSV file\n",
    "        min_date: Minimum date string\n",
    "        max_date: Maximum date string\n",
    "        \n",
    "    Returns:\n",
    "        Filtered dataframe and date limits dictionary\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading training data from {data_path}\")\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not data_path.exists():\n",
    "        logger.warning(f\"Data file not found at {data_path}. Attempting DVC pull...\")\n",
    "        os.system(\"dvc pull\")\n",
    "    \n",
    "    data = pd.read_csv(data_path)\n",
    "    logger.info(f\"Loaded {len(data):,} rows, {len(data.columns)} columns\")\n",
    "    \n",
    "    # Parse dates\n",
    "    max_date_parsed = pd.to_datetime(max_date or datetime.datetime.now()).date()\n",
    "    min_date_parsed = pd.to_datetime(min_date).date()\n",
    "    \n",
    "    # Apply date filter if date_part column exists\n",
    "    if \"date_part\" in data.columns:\n",
    "        data[\"date_part\"] = pd.to_datetime(data[\"date_part\"]).dt.date\n",
    "        data = data[\n",
    "            (data[\"date_part\"] >= min_date_parsed) & \n",
    "            (data[\"date_part\"] <= max_date_parsed)\n",
    "        ].reset_index(drop=True)\n",
    "        \n",
    "        actual_min = data[\"date_part\"].min()\n",
    "        actual_max = data[\"date_part\"].max()\n",
    "        \n",
    "        logger.info(f\"Date filtered: {actual_min} to {actual_max}\")\n",
    "        logger.info(f\"Total rows after filtering: {len(data):,}\")\n",
    "        \n",
    "        date_limits = {\"min_date\": str(actual_min), \"max_date\": str(actual_max)}\n",
    "    else:\n",
    "        logger.warning(\"No 'date_part' column found; skipping date filtering\")\n",
    "        date_limits = {\"min_date\": min_date, \"max_date\": max_date}\n",
    "    \n",
    "    return data, date_limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40f8bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================================\n",
    "# # EDA ANALYSIS FUNCTIONS\n",
    "# # =============================================================================\n",
    "# def analyze_missing_values(data: pd.DataFrame) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Analyze and report missing values.\n",
    "    \n",
    "#     Args:\n",
    "#         data: Input dataframe\n",
    "        \n",
    "#     Returns:\n",
    "#         Series of columns with missing value counts\n",
    "#     \"\"\"\n",
    "#     missing = data.isnull().sum()\n",
    "#     missing = missing[missing > 0].sort_values(ascending=False)\n",
    "    \n",
    "#     if not missing.empty:\n",
    "#         logger.info(f\"Found {len(missing)} columns with missing values\")\n",
    "#         for col, count in missing.head(10).items():\n",
    "#             pct = 100 * count / len(data)\n",
    "#             logger.info(f\"  {col}: {count:,} ({pct:.2f}%)\")\n",
    "#     else:\n",
    "#         logger.info(\"No missing values detected ✅\")\n",
    "    \n",
    "#     return missing\n",
    "\n",
    "# def analyze_numeric_columns(data: pd.DataFrame) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Comprehensive numeric column analysis.\n",
    "    \n",
    "#     Args:\n",
    "#         data: Input dataframe\n",
    "        \n",
    "#     Returns:\n",
    "#         Descriptive statistics dataframe\n",
    "#     \"\"\"\n",
    "#     numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "#     logger.info(f\"Analyzing {len(numeric_cols)} numeric columns\")\n",
    "    \n",
    "#     if not numeric_cols:\n",
    "#         logger.warning(\"No numeric columns found\")\n",
    "#         return pd.DataFrame()\n",
    "    \n",
    "#     # Descriptive statistics\n",
    "#     num_descr = data[numeric_cols].describe().T\n",
    "#     num_descr['missing'] = data[numeric_cols].isnull().sum()\n",
    "#     num_descr = num_descr[['count', 'missing', 'mean', 'std', 'min', '25%', '50%', '75%', 'max']]\n",
    "    \n",
    "#     # Save to CSV\n",
    "#     output_path = Config.ARTIFACTS_DIR / \"numeric_describe.csv\"\n",
    "#     num_descr.to_csv(output_path)\n",
    "#     logger.info(f\"Saved numeric summary to {output_path}\")\n",
    "    \n",
    "#     return num_descr\n",
    "\n",
    "# def plot_numeric_distributions(data: pd.DataFrame, max_cols: int = 20) -> None:\n",
    "#     \"\"\"\n",
    "#     Generate histograms for numeric columns.\n",
    "    \n",
    "#     Args:\n",
    "#         data: Input dataframe\n",
    "#         max_cols: Maximum number of columns to plot\n",
    "#     \"\"\"\n",
    "#     numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "#     cols_to_plot = numeric_cols[:max_cols]\n",
    "    \n",
    "#     logger.info(f\"Generating histograms for {len(cols_to_plot)} numeric columns\")\n",
    "    \n",
    "#     for col in cols_to_plot:\n",
    "#         fig, ax = plt.subplots(figsize=(10, 6))\n",
    "#         try:\n",
    "#             data[col].dropna().hist(bins=50, edgecolor='black', ax=ax)\n",
    "#             ax.set_title(f'Distribution of {col}')\n",
    "#             ax.set_xlabel(col)\n",
    "#             ax.set_ylabel('Frequency')\n",
    "#             save_plot(fig, f\"hist_{col}.png\")\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Failed to plot histogram for {col}: {e}\")\n",
    "#             plt.close(fig)\n",
    "\n",
    "# def plot_correlation_heatmap(data: pd.DataFrame) -> None:\n",
    "#     \"\"\"\n",
    "#     Generate correlation heatmap for numeric columns.\n",
    "    \n",
    "#     Args:\n",
    "#         data: Input dataframe\n",
    "#     \"\"\"\n",
    "#     numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    \n",
    "#     if len(numeric_cols) < 2:\n",
    "#         logger.warning(\"Need at least 2 numeric columns for correlation heatmap\")\n",
    "#         return\n",
    "    \n",
    "#     logger.info(\"Generating correlation heatmap\")\n",
    "    \n",
    "#     try:\n",
    "#         corr = data[numeric_cols].corr()\n",
    "        \n",
    "#         fig, ax = plt.subplots(figsize=(min(14, 1 + len(numeric_cols)), \n",
    "#                                          min(12, 1 + len(numeric_cols))))\n",
    "#         sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0, \n",
    "#                     cbar_kws={'shrink': 0.8}, ax=ax)\n",
    "#         ax.set_title('Correlation Heatmap', fontsize=16, pad=20)\n",
    "#         save_plot(fig, \"correlation_heatmap.png\")\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Failed to create correlation heatmap: {e}\")\n",
    "\n",
    "# def analyze_categorical_columns(data: pd.DataFrame) -> List[Dict]:\n",
    "#     \"\"\"\n",
    "#     Analyze categorical columns and save summary.\n",
    "    \n",
    "#     Args:\n",
    "#         data: Input dataframe\n",
    "        \n",
    "#     Returns:\n",
    "#         List of dictionaries with categorical summaries\n",
    "#     \"\"\"\n",
    "#     cat_cols = data.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "#     logger.info(f\"Analyzing {len(cat_cols)} categorical columns\")\n",
    "    \n",
    "#     if not cat_cols:\n",
    "#         logger.warning(\"No categorical columns found\")\n",
    "#         return []\n",
    "    \n",
    "#     cat_summary = []\n",
    "#     for col in cat_cols:\n",
    "#         top_values = data[col].value_counts(dropna=False).head(Config.MAX_CAT_TOP)\n",
    "#         unique_count = data[col].nunique(dropna=True)\n",
    "#         missing_count = data[col].isnull().sum()\n",
    "        \n",
    "#         cat_summary.append({\n",
    "#             'column': col,\n",
    "#             'unique': unique_count,\n",
    "#             'missing': missing_count,\n",
    "#             'top_values': {str(k): int(v) for k, v in top_values.to_dict().items()}\n",
    "#         })\n",
    "        \n",
    "#         logger.info(f\"  {col}: {unique_count} unique values, {missing_count} missing\")\n",
    "    \n",
    "#     # Save summary\n",
    "#     cat_summary_safe = to_json_safe(cat_summary)\n",
    "#     output_path = Config.ARTIFACTS_DIR / \"categorical_summary.json\"\n",
    "#     with open(output_path, 'w') as f:\n",
    "#         json.dump(cat_summary_safe, f, indent=2)\n",
    "#     logger.info(f\"Saved categorical summary to {output_path}\")\n",
    "    \n",
    "#     return cat_summary\n",
    "\n",
    "# def plot_categorical_distributions(data: pd.DataFrame) -> None:\n",
    "#     \"\"\"\n",
    "#     Generate bar plots for categorical columns.\n",
    "    \n",
    "#     Args:\n",
    "#         data: Input dataframe\n",
    "#     \"\"\"\n",
    "#     cat_cols = data.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "    \n",
    "#     logger.info(f\"Generating bar plots for {len(cat_cols)} categorical columns\")\n",
    "    \n",
    "#     for col in cat_cols:\n",
    "#         fig, ax = plt.subplots(figsize=(10, 6))\n",
    "#         try:\n",
    "#             top = data[col].value_counts(dropna=False).nlargest(Config.MAX_CAT_TOP)\n",
    "#             top.plot(kind='barh', ax=ax)\n",
    "#             ax.set_title(f'Top {Config.MAX_CAT_TOP} Categories in {col}')\n",
    "#             ax.set_xlabel('Count')\n",
    "#             ax.set_ylabel('Category')\n",
    "#             save_plot(fig, f\"cat_{col}.png\")\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Failed to plot categories for {col}: {e}\")\n",
    "#             plt.close(fig)\n",
    "\n",
    "# def plot_missingness(data: pd.DataFrame) -> None:\n",
    "#     \"\"\"\n",
    "#     Visualize missing data patterns.\n",
    "    \n",
    "#     Args:\n",
    "#         data: Input dataframe\n",
    "#     \"\"\"\n",
    "#     logger.info(\"Generating missingness visualizations\")\n",
    "    \n",
    "#     if not HAS_MISSINGNO:\n",
    "#         logger.warning(\"missingno library not available, using fallback\")\n",
    "#         missing = data.isnull().sum()\n",
    "#         missing = missing[missing > 0].sort_values(ascending=False)\n",
    "        \n",
    "#         if not missing.empty:\n",
    "#             fig, ax = plt.subplots(figsize=(10, 6))\n",
    "#             missing.plot(kind='barh', ax=ax)\n",
    "#             ax.set_title('Missing Value Counts')\n",
    "#             ax.set_xlabel('Count')\n",
    "#             save_plot(fig, \"missing_counts.png\")\n",
    "#         return\n",
    "    \n",
    "#     try:\n",
    "#         # Missingness matrix\n",
    "#         fig = msno.matrix(data, figsize=(12, 6)).get_figure()\n",
    "#         fig.suptitle('Missingness Matrix')\n",
    "#         save_plot(fig, \"missingness_matrix.png\")\n",
    "        \n",
    "#         # Missingness heatmap\n",
    "#         fig = msno.heatmap(data, figsize=(10, 8)).get_figure()\n",
    "#         fig.suptitle('Missingness Correlation')\n",
    "#         save_plot(fig, \"missingness_heatmap.png\")\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"missingno visualization failed: {e}\")\n",
    "\n",
    "# def analyze_time_series(data: pd.DataFrame) -> None:\n",
    "#     \"\"\"\n",
    "#     Analyze temporal patterns if date column exists.\n",
    "    \n",
    "#     Args:\n",
    "#         data: Input dataframe\n",
    "#     \"\"\"\n",
    "#     if 'date_part' not in data.columns:\n",
    "#         logger.info(\"No 'date_part' column found; skipping time series analysis\")\n",
    "#         return\n",
    "    \n",
    "#     logger.info(\"Analyzing time series patterns\")\n",
    "    \n",
    "#     try:\n",
    "#         daily_counts = data.groupby('date_part').size()\n",
    "        \n",
    "#         fig, ax = plt.subplots(figsize=(12, 6))\n",
    "#         daily_counts.plot(ax=ax, marker='o')\n",
    "#         ax.set_title('Rows Over Time')\n",
    "#         ax.set_xlabel('Date')\n",
    "#         ax.set_ylabel('Row Count')\n",
    "#         ax.grid(True, alpha=0.3)\n",
    "#         save_plot(fig, \"time_series.png\")\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Time series analysis failed: {e}\")\n",
    "\n",
    "# def identify_target_candidates(data: pd.DataFrame) -> List[str]:\n",
    "#     \"\"\"\n",
    "#     Identify potential target columns based on naming patterns.\n",
    "    \n",
    "#     Args:\n",
    "#         data: Input dataframe\n",
    "        \n",
    "#     Returns:\n",
    "#         List of potential target column names\n",
    "#     \"\"\"\n",
    "#     target_keywords = ['target', 'label', 'y', 'lead', 'outcome', 'prediction']\n",
    "#     candidates = [\n",
    "#         col for col in data.columns \n",
    "#         if any(keyword in col.lower() for keyword in target_keywords)\n",
    "#     ]\n",
    "    \n",
    "#     if candidates:\n",
    "#         logger.info(f\"Identified target candidates: {candidates}\")\n",
    "#     else:\n",
    "#         logger.info(\"No obvious target columns found\")\n",
    "#         # Show top variance columns as fallback\n",
    "#         numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "#         if len(numeric_cols) > 0:\n",
    "#             variances = data[numeric_cols].var().sort_values(ascending=False).head(5)\n",
    "#             logger.info(f\"Top variance columns:\\n{variances}\")\n",
    "    \n",
    "#     return candidates\n",
    "\n",
    "# # =============================================================================\n",
    "# # MAIN PIPELINE\n",
    "# # =============================================================================\n",
    "# def run_eda_pipeline():\n",
    "#     \"\"\"Execute complete EDA pipeline.\"\"\"\n",
    "#     logger.info(\"=\"*70)\n",
    "#     logger.info(\"Starting EDA Pipeline\")\n",
    "#     logger.info(\"=\"*70)\n",
    "    \n",
    "#     # Setup\n",
    "#     Config.setup_directories()\n",
    "    \n",
    "#     # Load data\n",
    "#     data, date_limits = load_and_filter_data(\n",
    "#         Config.DATA_PATH,\n",
    "#         Config.MIN_DATE,\n",
    "#         Config.MAX_DATE\n",
    "#     )\n",
    "    \n",
    "#     # Save date limits\n",
    "#     with open(Config.ARTIFACTS_DIR / \"date_limits.json\", \"w\") as f:\n",
    "#         json.dump(date_limits, f, indent=2)\n",
    "    \n",
    "#     # Display basic info\n",
    "#     logger.info(f\"Data shape: {data.shape}\")\n",
    "#     logger.info(f\"Columns: {list(data.columns)}\")\n",
    "    \n",
    "#     # Missing values analysis\n",
    "#     missing = analyze_missing_values(data)\n",
    "    \n",
    "#     # Numeric analysis\n",
    "#     logger.info(\"\\n\" + \"=\"*70)\n",
    "#     logger.info(\"Numeric Column Analysis\")\n",
    "#     logger.info(\"=\"*70)\n",
    "#     num_stats = analyze_numeric_columns(data)\n",
    "#     if not num_stats.empty:\n",
    "#         plot_numeric_distributions(data, Config.MAX_HIST_COLS)\n",
    "#         plot_correlation_heatmap(data)\n",
    "    \n",
    "#     # Categorical analysis\n",
    "#     logger.info(\"\\n\" + \"=\"*70)\n",
    "#     logger.info(\"Categorical Column Analysis\")\n",
    "#     logger.info(\"=\"*70)\n",
    "#     cat_summary = analyze_categorical_columns(data)\n",
    "#     if cat_summary:\n",
    "#         plot_categorical_distributions(data)\n",
    "    \n",
    "#     # Missingness visualization\n",
    "#     logger.info(\"\\n\" + \"=\"*70)\n",
    "#     logger.info(\"Missingness Analysis\")\n",
    "#     logger.info(\"=\"*70)\n",
    "#     plot_missingness(data)\n",
    "    \n",
    "#     # Time series analysis\n",
    "#     logger.info(\"\\n\" + \"=\"*70)\n",
    "#     logger.info(\"Time Series Analysis\")\n",
    "#     logger.info(\"=\"*70)\n",
    "#     analyze_time_series(data)\n",
    "    \n",
    "#     # Target identification\n",
    "#     logger.info(\"\\n\" + \"=\"*70)\n",
    "#     logger.info(\"Target Column Identification\")\n",
    "#     logger.info(\"=\"*70)\n",
    "#     target_candidates = identify_target_candidates(data)\n",
    "    \n",
    "#     logger.info(\"\\n\" + \"=\"*70)\n",
    "#     logger.info(\"EDA Pipeline Complete ✅\")\n",
    "#     logger.info(f\"Artifacts saved to: {Config.ARTIFACTS_DIR}\")\n",
    "#     logger.info(f\"Plots saved to: {Config.PLOTS_DIR}\")\n",
    "#     logger.info(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a19a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v2 Data analysis \n",
    "# =============================================================================\n",
    "# MODERN MLOPS EDA VISUALS: ONLY THE ESSENTIALS\n",
    "# =============================================================================\n",
    "\n",
    "def plot_missingness(data: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Visualize missing data patterns.\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating missingness visualizations\")\n",
    "    try:\n",
    "        fig = msno.matrix(data, figsize=(12, 6)).get_figure()\n",
    "        fig.suptitle('Missingness Matrix')\n",
    "        save_plot(fig, \"missingness_matrix.png\")\n",
    "    except Exception as e:\n",
    "        logger.warning(\"missingno failed, using bar fallback: %s\", e)\n",
    "        missing = data.isnull().sum()\n",
    "        missing = missing[missing > 0].sort_values(ascending=False)\n",
    "        if not missing.empty:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            missing.plot(kind='barh', ax=ax)\n",
    "            ax.set_title('Missing Value Counts')\n",
    "            ax.set_xlabel('Count')\n",
    "            save_plot(fig, \"missing_counts.png\")\n",
    "        else:\n",
    "            logger.info(\"No missing values detected ✅\")\n",
    "\n",
    "\n",
    "def plot_correlation_heatmap(data: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Correlation heatmap for key numeric columns (skip ID-like).\n",
    "    \"\"\"\n",
    "    # Only use relevant (non-id) numeric columns\n",
    "    numeric_cols = [c for c in data.select_dtypes(include=['int64', 'float64']).columns if 'id' not in c.lower()]\n",
    "    if len(numeric_cols) < 2:\n",
    "        logger.info(\"Not enough numeric columns for correlation heatmap\")\n",
    "        return\n",
    "    try:\n",
    "        corr = data[numeric_cols].corr()\n",
    "        fig, ax = plt.subplots(figsize=(min(14, 1+len(numeric_cols)), min(12, 1+len(numeric_cols))))\n",
    "        sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=ax)\n",
    "        ax.set_title('Correlation Heatmap')\n",
    "        save_plot(fig, \"correlation_heatmap.png\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to create correlation heatmap: %s\", e)\n",
    "\n",
    "\n",
    "def plot_target_distribution(data: pd.DataFrame, target_col: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot for the key target/label column (for classification/regression). \n",
    "    \"\"\"\n",
    "    if target_col not in data.columns:\n",
    "        logger.warning(f\"{target_col} not in columns, skipping target plot.\")\n",
    "        return\n",
    "    try:\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        if data[target_col].dtype in ['object', 'category', 'bool']:\n",
    "            data[target_col].value_counts(dropna=False).plot(kind='bar', ax=ax)\n",
    "            ax.set_title(f\"Class Distribution: {target_col}\")\n",
    "        else:\n",
    "            data[target_col].hist(bins=25, ax=ax)\n",
    "            ax.set_title(f\"Distribution: {target_col}\")\n",
    "        save_plot(fig, f\"target_{target_col}_dist.png\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not plot target {target_col}: {e}\")\n",
    "\n",
    "\n",
    "def plot_key_feature_distributions(data: pd.DataFrame, key_features: list) -> None:\n",
    "    \"\"\"\n",
    "    Plot distribution for selected, relevant features only (not all columns).\n",
    "    \"\"\"\n",
    "    for col in key_features:\n",
    "        if col in data.columns:\n",
    "            try:\n",
    "                fig, ax = plt.subplots(figsize=(8, 5))\n",
    "                if data[col].dtype in ['object', 'category', 'bool']:\n",
    "                    data[col].value_counts(dropna=False).head(10).plot(kind='barh', ax=ax)\n",
    "                    ax.set_title(f'Top Categories in {col}')\n",
    "                else:\n",
    "                    data[col].hist(bins=25, ax=ax)\n",
    "                    ax.set_title(f'Distribution of {col}')\n",
    "                save_plot(fig, f\"{col}_dist.png\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Could not plot feature {col}: {e}\")\n",
    "\n",
    "\n",
    "def plot_time_series_row_count(data: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Row count over time, using a date column for drift checks.\n",
    "    \"\"\"\n",
    "    if 'date_part' in data.columns:\n",
    "        try:\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            data.groupby('date_part').size().plot(ax=ax, marker='o')\n",
    "            ax.set_title('Rows Over Time')\n",
    "            ax.set_ylabel('Row Count')\n",
    "            ax.set_xlabel('Date')\n",
    "            save_plot(fig, \"time_series_row_count.png\")\n",
    "        except Exception as e:\n",
    "            logger.error(\"Could not plot time series row count: %s\", e)\n",
    "            \n",
    "# =====================================================================\n",
    "# Replace full-featured plotting blocks in your pipeline with ONLY these\n",
    "# =====================================================================\n",
    "def run_eda_pipeline():\n",
    "    logger.info(\"=\"*70)\n",
    "    logger.info(\"Starting Minimal EDA Pipeline For MLOps\")\n",
    "    logger.info(\"=\"*70)\n",
    "\n",
    "    Config.setup_directories()\n",
    "    data, date_limits = load_and_filter_data(Config.DATA_PATH, Config.MIN_DATE, Config.MAX_DATE)\n",
    "    with open(Config.ARTIFACTS_DIR / \"date_limits.json\", \"w\") as f:\n",
    "        json.dump(date_limits, f, indent=2)\n",
    "\n",
    "    logger.info(f\"Data shape: {data.shape}\")\n",
    "    logger.info(f\"Columns: {list(data.columns)}\")\n",
    "    missing = analyze_missing_values(data)\n",
    "\n",
    "    # === ESSENTIAL VISUALS ONLY ===\n",
    "    plot_missingness(data)\n",
    "    plot_correlation_heatmap(data)\n",
    "    plot_target_distribution(data, target_col='lead_indicator')  # or your actual target\n",
    "    # Pass the most important features for your use case here:\n",
    "    plot_key_feature_distributions(data, key_features=['customer_group', 'time_spent', 'n_visits', 'purchases'])\n",
    "    plot_time_series_row_count(data)\n",
    "\n",
    "    logger.info(\"=\"*70)\n",
    "    logger.info(\"Minimal EDA Pipeline Complete ✅\")\n",
    "    logger.info(f\"Artifacts saved to: {Config.ARTIFACTS_DIR / 'plots'}\")\n",
    "    logger.info(\"=\"*70)\n",
    "    \n",
    "\n",
    "def analyze_missing_values(data: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Analyze and report missing values.\n",
    "    \n",
    "    Args:\n",
    "        data: Input dataframe\n",
    "        \n",
    "    Returns:\n",
    "        Series of columns with missing value counts\n",
    "    \"\"\"\n",
    "    missing = data.isnull().sum()\n",
    "    missing = missing[missing > 0].sort_values(ascending=False)\n",
    "    \n",
    "    if not missing.empty:\n",
    "        logger.info(f\"Found {len(missing)} columns with missing values\")\n",
    "        for col, count in missing.head(10).items():\n",
    "            pct = 100 * count / len(data)\n",
    "            logger.info(f\"  {col}: {count:,} ({pct:.2f}%)\")\n",
    "    else:\n",
    "        logger.info(\"No missing values detected ✅\")\n",
    "    \n",
    "    return missing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78bb18b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# Replace full-featured plotting blocks in your pipeline with ONLY these\n",
    "# =====================================================================\n",
    "def run_eda_pipeline():\n",
    "    logger.info(\"=\"*70)\n",
    "    logger.info(\"Starting Minimal EDA Pipeline For MLOps\")\n",
    "    logger.info(\"=\"*70)\n",
    "\n",
    "    Config.setup_directories()\n",
    "    data, date_limits = load_and_filter_data(Config.DATA_PATH, Config.MIN_DATE, Config.MAX_DATE)\n",
    "    with open(Config.ARTIFACTS_DIR / \"date_limits.json\", \"w\") as f:\n",
    "        json.dump(date_limits, f, indent=2)\n",
    "\n",
    "    logger.info(f\"Data shape: {data.shape}\")\n",
    "    logger.info(f\"Columns: {list(data.columns)}\")\n",
    "    missing = analyze_missing_values(data)\n",
    "\n",
    "    # === ESSENTIAL VISUALS ONLY ===\n",
    "    plot_missingness(data)\n",
    "    plot_correlation_heatmap(data)\n",
    "    plot_target_distribution(data, target_col='lead_indicator')  # or your actual target\n",
    "    # Pass the most important features for your use case here:\n",
    "    plot_key_feature_distributions(data, key_features=['customer_group', 'time_spent', 'n_visits', 'purchases'])\n",
    "    plot_time_series_row_count(data)\n",
    "\n",
    "    logger.info(\"=\"*70)\n",
    "    logger.info(\"Minimal EDA Pipeline Complete ✅\")\n",
    "    logger.info(f\"Artifacts saved to: {Config.ARTIFACTS_DIR / 'plots'}\")\n",
    "    logger.info(\"=\"*70)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61b4585c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 13:43:47 - eda_pipeline - INFO - ======================================================================\n",
      "2025-10-23 13:43:47 - eda_pipeline - INFO - Starting Minimal EDA Pipeline For MLOps\n",
      "2025-10-23 13:43:47 - eda_pipeline - INFO - ======================================================================\n",
      "2025-10-23 13:43:47 - eda_pipeline - INFO - Created artifacts directories: artifacts, artifacts/plots\n",
      "2025-10-23 13:43:47 - eda_pipeline - INFO - Loading training data from artifacts/raw_data.csv\n",
      "2025-10-23 13:43:47 - eda_pipeline - INFO - Loaded 12,345 rows, 19 columns\n",
      "2025-10-23 13:43:47 - eda_pipeline - INFO - Date filtered: 2024-01-01 to 2024-01-31\n",
      "2025-10-23 13:43:47 - eda_pipeline - INFO - Total rows after filtering: 12,345\n",
      "2025-10-23 13:43:47 - eda_pipeline - INFO - Data shape: (12345, 19)\n",
      "2025-10-23 13:43:47 - eda_pipeline - INFO - Columns: ['lead_id', 'lead_indicator', 'date_part', 'is_active', 'marketing_consent', 'first_booking', 'existing_customer', 'last_seen', 'source', 'domain', 'country', 'visited_learn_more_before_booking', 'visited_faq', 'purchases', 'time_spent', 'customer_group', 'onboarding', 'customer_code', 'n_visits']\n",
      "2025-10-23 13:43:47 - eda_pipeline - INFO - Found 2 columns with missing values\n",
      "2025-10-23 13:43:47 - eda_pipeline - INFO -   lead_indicator: 592 (4.80%)\n",
      "2025-10-23 13:43:47 - eda_pipeline - INFO -   customer_code: 51 (0.41%)\n",
      "2025-10-23 13:43:47 - eda_pipeline - INFO - Generating missingness visualizations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-23 13:43:49 - eda_pipeline - INFO - ======================================================================\n",
      "2025-10-23 13:43:49 - eda_pipeline - INFO - Minimal EDA Pipeline Complete ✅\n",
      "2025-10-23 13:43:49 - eda_pipeline - INFO - Artifacts saved to: artifacts/plots\n",
      "2025-10-23 13:43:49 - eda_pipeline - INFO - ======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXECUTION\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    run_eda_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLOPS venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
