{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e781124",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MLOps EDA Pipeline\n",
    "==================\n",
    "Exploratory Data Analysis for training data validation and quality checks.\n",
    "Produces artifacts for downstream pipeline stages.\n",
    "\"\"\"\n",
    "\n",
    "# =============================================================================\n",
    "# IMPORTS AND CONFIGURATION\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Non-interactive backend to prevent memory leaks\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "try:\n",
    "    import missingno as msno\n",
    "    HAS_MISSINGNO = True\n",
    "except ImportError:\n",
    "    HAS_MISSINGNO = False\n",
    "\n",
    "# Configure warnings and pandas display\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.float_format', lambda x: \"%.3f\" % x)\n",
    "\n",
    "# Plotting defaults\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams.update({\"figure.figsize\": (10, 6), \"figure.dpi\": 120})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d9744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# LOGGING SETUP\n",
    "# =============================================================================\n",
    "def setup_logger(name: str = \"eda_pipeline\", level: int = logging.INFO) -> logging.Logger:\n",
    "    \"\"\"\n",
    "    Configure logger for the EDA pipeline.\n",
    "    \n",
    "    Args:\n",
    "        name: Logger name\n",
    "        level: Logging level\n",
    "        \n",
    "    Returns:\n",
    "        Configured logger instance\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    \n",
    "    # Console handler\n",
    "    if not logger.handlers:\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S'\n",
    "        )\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "logger = setup_logger()\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION\n",
    "# =============================================================================\n",
    "class Config:\n",
    "    \"\"\"Configuration constants for the EDA pipeline.\"\"\"\n",
    "    PLOTS_DIR = Path(\"plots\")\n",
    "    DATA_PATH = Path(\"../data/raw/raw_data.csv\")\n",
    "    \n",
    "    # Date range (can be parameterized)\n",
    "    MAX_DATE = \"2024-03-31\"\n",
    "    MIN_DATE = \"2023-12-01\"\n",
    "    \n",
    "    # EDA parameters\n",
    "    MAX_HIST_COLS = 20  # Limit histograms to prevent memory issues\n",
    "    MAX_CAT_TOP = 10     # Top categories to display\n",
    "    \n",
    "    @classmethod\n",
    "    def setup_directories(cls):\n",
    "        \"\"\"Create necessary directories.\"\"\"\n",
    "        cls.PLOTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        logger.info(f\"Created plots directory: {cls.PLOTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1640dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "def to_json_safe(obj):\n",
    "    \"\"\"\n",
    "    Convert numpy/pandas types to JSON-serializable Python types.\n",
    "    \n",
    "    Args:\n",
    "        obj: Object to convert\n",
    "        \n",
    "    Returns:\n",
    "        JSON-safe version of the object\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {str(k): to_json_safe(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, (list, tuple)):\n",
    "        return [to_json_safe(v) for v in obj]\n",
    "    elif isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (pd.Timestamp, datetime.date, datetime.datetime)):\n",
    "        return obj.isoformat()\n",
    "    elif isinstance(obj, np.generic):\n",
    "        return obj.item()\n",
    "    elif isinstance(obj, (int, float, str, bool, type(None))):\n",
    "        return obj\n",
    "    else:\n",
    "        return str(obj)\n",
    "\n",
    "def save_plot(fig: plt.Figure, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Save matplotlib figure and properly close it to prevent memory leaks.\n",
    "    \n",
    "    Args:\n",
    "        fig: Matplotlib figure object\n",
    "        filename: Output filename\n",
    "    \"\"\"\n",
    "    try:\n",
    "        filepath = Config.PLOTS_DIR / filename\n",
    "        fig.savefig(filepath, bbox_inches='tight', dpi=120)\n",
    "        logger.debug(f\"Saved plot: {filename}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save plot {filename}: {e}\")\n",
    "    finally:\n",
    "        plt.close(fig)  # Critical for memory management\n",
    "\n",
    "def describe_numeric_col(x: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Calculate descriptive statistics for a numeric column.\n",
    "    \n",
    "    Args:\n",
    "        x: Pandas Series (numeric)\n",
    "        \n",
    "    Returns:\n",
    "        Series with descriptive stats\n",
    "    \"\"\"\n",
    "    return pd.Series({\n",
    "        \"Count\": x.count(),\n",
    "        \"Missing\": x.isnull().sum(),\n",
    "        \"Mean\": x.mean(),\n",
    "        \"Std\": x.std(),\n",
    "        \"Min\": x.min(),\n",
    "        \"25%\": x.quantile(0.25),\n",
    "        \"50%\": x.quantile(0.50),\n",
    "        \"75%\": x.quantile(0.75),\n",
    "        \"Max\": x.max()\n",
    "    })\n",
    "\n",
    "def impute_missing_values(x: pd.Series, method: str = \"mean\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Impute missing values in a column.\n",
    "    \n",
    "    Args:\n",
    "        x: Pandas Series\n",
    "        method: Imputation method (\"mean\", \"median\", \"mode\")\n",
    "        \n",
    "    Returns:\n",
    "        Series with imputed values\n",
    "    \"\"\"\n",
    "    if x.dtype in ['float64', 'int64']:\n",
    "        if method == \"mean\":\n",
    "            return x.fillna(x.mean())\n",
    "        elif method == \"median\":\n",
    "            return x.fillna(x.median())\n",
    "    \n",
    "    # For categorical/other types, use mode\n",
    "    if not x.mode().empty:\n",
    "        return x.fillna(x.mode()[0])\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11e3fc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING AND PREPROCESSING\n",
    "# =============================================================================\n",
    "def load_and_filter_data(\n",
    "    data_path: Path,\n",
    "    min_date: str,\n",
    "    max_date: str\n",
    ") -> Tuple[pd.DataFrame, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Load data and apply date filtering.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to the CSV file\n",
    "        min_date: Minimum date string\n",
    "        max_date: Maximum date string\n",
    "        \n",
    "    Returns:\n",
    "        Filtered dataframe and date limits dictionary\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading training data from {data_path}\")\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not data_path.exists():\n",
    "        logger.warning(f\"Data file not found at {data_path}. Attempting DVC pull...\")\n",
    "        os.system(\"dvc pull\")\n",
    "    \n",
    "    data = pd.read_csv(data_path)\n",
    "    logger.info(f\"Loaded {len(data):,} rows, {len(data.columns)} columns\")\n",
    "    \n",
    "    # Parse dates\n",
    "    max_date_parsed = pd.to_datetime(max_date or datetime.datetime.now()).date()\n",
    "    min_date_parsed = pd.to_datetime(min_date).date()\n",
    "    \n",
    "    # Apply date filter if date_part column exists\n",
    "    if \"date_part\" in data.columns:\n",
    "        data[\"date_part\"] = pd.to_datetime(data[\"date_part\"]).dt.date\n",
    "        data = data[\n",
    "            (data[\"date_part\"] >= min_date_parsed) & \n",
    "            (data[\"date_part\"] <= max_date_parsed)\n",
    "        ].reset_index(drop=True)\n",
    "        \n",
    "        actual_min = data[\"date_part\"].min()\n",
    "        actual_max = data[\"date_part\"].max()\n",
    "        \n",
    "        logger.info(f\"Date filtered: {actual_min} to {actual_max}\")\n",
    "        logger.info(f\"Total rows after filtering: {len(data):,}\")\n",
    "        \n",
    "        date_limits = {\"min_date\": str(actual_min), \"max_date\": str(actual_max)}\n",
    "    else:\n",
    "        logger.warning(\"No 'date_part' column found; skipping date filtering\")\n",
    "        date_limits = {\"min_date\": min_date, \"max_date\": max_date}\n",
    "    \n",
    "    return data, date_limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a19a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MLOPS EDA VISUALS: ONLY THE ESSENTIALS\n",
    "# =============================================================================\n",
    "\n",
    "def plot_missingness(data: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Visualize missing data patterns.\n",
    "    \"\"\"\n",
    "    logger.info(\"Generating missingness visualizations\")\n",
    "    try:\n",
    "        fig = msno.matrix(data, figsize=(12, 6)).get_figure()\n",
    "        fig.suptitle('Missingness Matrix')\n",
    "        save_plot(fig, \"missingness_matrix.png\")\n",
    "    except Exception as e:\n",
    "        logger.warning(\"missingno failed, using bar fallback: %s\", e)\n",
    "        missing = data.isnull().sum()\n",
    "        missing = missing[missing > 0].sort_values(ascending=False)\n",
    "        if not missing.empty:\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            missing.plot(kind='barh', ax=ax)\n",
    "            ax.set_title('Missing Value Counts')\n",
    "            ax.set_xlabel('Count')\n",
    "            save_plot(fig, \"missing_counts.png\")\n",
    "        else:\n",
    "            logger.info(\"No missing values detected ✅\")\n",
    "\n",
    "\n",
    "def plot_correlation_heatmap(data: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Correlation heatmap for key numeric columns (skip ID-like).\n",
    "    \"\"\"\n",
    "    # Only use relevant (non-id) numeric columns\n",
    "    numeric_cols = [c for c in data.select_dtypes(include=['int64', 'float64']).columns if 'id' not in c.lower()]\n",
    "    if len(numeric_cols) < 2:\n",
    "        logger.info(\"Not enough numeric columns for correlation heatmap\")\n",
    "        return\n",
    "    try:\n",
    "        corr = data[numeric_cols].corr()\n",
    "        fig, ax = plt.subplots(figsize=(min(14, 1+len(numeric_cols)), min(12, 1+len(numeric_cols))))\n",
    "        sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0, ax=ax)\n",
    "        ax.set_title('Correlation Heatmap')\n",
    "        save_plot(fig, \"correlation_heatmap.png\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to create correlation heatmap: %s\", e)\n",
    "\n",
    "\n",
    "def plot_target_distribution(data: pd.DataFrame, target_col: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot for the key target/label column (for classification/regression). \n",
    "    \"\"\"\n",
    "    if target_col not in data.columns:\n",
    "        logger.warning(f\"{target_col} not in columns, skipping target plot.\")\n",
    "        return\n",
    "    try:\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "        if data[target_col].dtype in ['object', 'category', 'bool']:\n",
    "            data[target_col].value_counts(dropna=False).plot(kind='bar', ax=ax)\n",
    "            ax.set_title(f\"Class Distribution: {target_col}\")\n",
    "        else:\n",
    "            data[target_col].hist(bins=25, ax=ax)\n",
    "            ax.set_title(f\"Distribution: {target_col}\")\n",
    "        save_plot(fig, f\"target_{target_col}_dist.png\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Could not plot target {target_col}: {e}\")\n",
    "\n",
    "\n",
    "def plot_key_feature_distributions(data: pd.DataFrame, key_features: list) -> None:\n",
    "    \"\"\"\n",
    "    Plot distribution for selected, relevant features only (not all columns).\n",
    "    \"\"\"\n",
    "    for col in key_features:\n",
    "        if col in data.columns:\n",
    "            try:\n",
    "                fig, ax = plt.subplots(figsize=(8, 5))\n",
    "                if data[col].dtype in ['object', 'category', 'bool']:\n",
    "                    data[col].value_counts(dropna=False).head(10).plot(kind='barh', ax=ax)\n",
    "                    ax.set_title(f'Top Categories in {col}')\n",
    "                else:\n",
    "                    data[col].hist(bins=25, ax=ax)\n",
    "                    ax.set_title(f'Distribution of {col}')\n",
    "                save_plot(fig, f\"{col}_dist.png\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Could not plot feature {col}: {e}\")\n",
    "\n",
    "\n",
    "def plot_time_series_row_count(data: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Row count over time, using a date column for drift checks.\n",
    "    \"\"\"\n",
    "    if 'date_part' in data.columns:\n",
    "        try:\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            data.groupby('date_part').size().plot(ax=ax, marker='o')\n",
    "            ax.set_title('Rows Over Time')\n",
    "            ax.set_ylabel('Row Count')\n",
    "            ax.set_xlabel('Date')\n",
    "            save_plot(fig, \"time_series_row_count.png\")\n",
    "        except Exception as e:\n",
    "            logger.error(\"Could not plot time series row count: %s\", e)\n",
    "            \n",
    "# =====================================================================\n",
    "# Replace full-featured plotting blocks in your pipeline with ONLY these\n",
    "# =====================================================================\n",
    "def run_eda_pipeline():\n",
    "    logger.info(\"=\"*70)\n",
    "    logger.info(\"Starting Minimal EDA Pipeline For MLOps\")\n",
    "    logger.info(\"=\"*70)\n",
    "\n",
    "    Config.setup_directories()\n",
    "    data, date_limits = load_and_filter_data(Config.DATA_PATH, Config.MIN_DATE, Config.MAX_DATE)\n",
    "\n",
    "    logger.info(f\"Data shape: {data.shape}\")\n",
    "    logger.info(f\"Columns: {list(data.columns)}\")\n",
    "    missing = analyze_missing_values(data)\n",
    "\n",
    "    # === ESSENTIAL VISUALS ONLY ===\n",
    "    plot_missingness(data)\n",
    "    plot_correlation_heatmap(data)\n",
    "    plot_target_distribution(data, target_col='lead_indicator')  # or your actual target\n",
    "    # Pass the most important features for your use case here:\n",
    "    plot_key_feature_distributions(data, key_features=['customer_group', 'time_spent', 'n_visits', 'purchases'])\n",
    "    plot_time_series_row_count(data)\n",
    "\n",
    "    logger.info(\"=\"*70)\n",
    "    logger.info(\"Minimal EDA Pipeline Complete ✅\")\n",
    "    logger.info(f\"Plots saved to: {Config.PLOTS_DIR}\")\n",
    "    logger.info(\"=\"*70)\n",
    "    \n",
    "\n",
    "def analyze_missing_values(data: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Analyze and report missing values.\n",
    "    \n",
    "    Args:\n",
    "        data: Input dataframe\n",
    "        \n",
    "    Returns:\n",
    "        Series of columns with missing value counts\n",
    "    \"\"\"\n",
    "    missing = data.isnull().sum()\n",
    "    missing = missing[missing > 0].sort_values(ascending=False)\n",
    "    \n",
    "    if not missing.empty:\n",
    "        logger.info(f\"Found {len(missing)} columns with missing values\")\n",
    "        for col, count in missing.head(10).items():\n",
    "            pct = 100 * count / len(data)\n",
    "            logger.info(f\"  {col}: {count:,} ({pct:.2f}%)\")\n",
    "    else:\n",
    "        logger.info(\"No missing values detected ✅\")\n",
    "    \n",
    "    return missing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78bb18b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# Replace full-featured plotting blocks in your pipeline with ONLY these\n",
    "# =====================================================================\n",
    "def run_eda_pipeline():\n",
    "    logger.info(\"=\"*70)\n",
    "    logger.info(\"Starting Minimal EDA Pipeline For MLOps\")\n",
    "    logger.info(\"=\"*70)\n",
    "\n",
    "    Config.setup_directories()\n",
    "    data, date_limits = load_and_filter_data(Config.DATA_PATH, Config.MIN_DATE, Config.MAX_DATE)\n",
    "\n",
    "    logger.info(f\"Data shape: {data.shape}\")\n",
    "    logger.info(f\"Columns: {list(data.columns)}\")\n",
    "    missing = analyze_missing_values(data)\n",
    "\n",
    "    # === ESSENTIAL VISUALS ONLY ===\n",
    "    plot_missingness(data)\n",
    "    plot_correlation_heatmap(data)\n",
    "    plot_target_distribution(data, target_col='lead_indicator')  # or your actual target\n",
    "    # Pass the most important features for your use case here:\n",
    "    plot_key_feature_distributions(data, key_features=['customer_group', 'time_spent', 'n_visits', 'purchases'])\n",
    "    plot_time_series_row_count(data)\n",
    "\n",
    "    logger.info(\"=\"*70)\n",
    "    logger.info(\"Minimal EDA Pipeline Complete ✅\")\n",
    "    logger.info(f\"Plots saved to: {Config.PLOTS_DIR}\")\n",
    "    logger.info(\"=\"*70)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61b4585c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-09 21:33:28 - eda_pipeline - INFO - ======================================================================\n",
      "2025-12-09 21:33:28 - eda_pipeline - INFO - Starting Minimal EDA Pipeline For MLOps\n",
      "2025-12-09 21:33:28 - eda_pipeline - INFO - ======================================================================\n",
      "2025-12-09 21:33:28 - eda_pipeline - INFO - Created plots directory: notebooks/plots\n",
      "2025-12-09 21:33:28 - eda_pipeline - INFO - Loading training data from ../data/raw/raw_data.csv\n",
      "2025-12-09 21:33:28 - eda_pipeline - INFO - Loaded 12,345 rows, 19 columns\n",
      "2025-12-09 21:33:28 - eda_pipeline - INFO - Date filtered: 2024-01-01 to 2024-01-31\n",
      "2025-12-09 21:33:28 - eda_pipeline - INFO - Total rows after filtering: 12,345\n",
      "2025-12-09 21:33:28 - eda_pipeline - INFO - Data shape: (12345, 19)\n",
      "2025-12-09 21:33:28 - eda_pipeline - INFO - Columns: ['lead_id', 'lead_indicator', 'date_part', 'is_active', 'marketing_consent', 'first_booking', 'existing_customer', 'last_seen', 'source', 'domain', 'country', 'visited_learn_more_before_booking', 'visited_faq', 'purchases', 'time_spent', 'customer_group', 'onboarding', 'customer_code', 'n_visits']\n",
      "2025-12-09 21:33:28 - eda_pipeline - INFO - Found 2 columns with missing values\n",
      "2025-12-09 21:33:28 - eda_pipeline - INFO -   lead_indicator: 592 (4.80%)\n",
      "2025-12-09 21:33:28 - eda_pipeline - INFO -   customer_code: 51 (0.41%)\n",
      "2025-12-09 21:33:28 - eda_pipeline - INFO - Generating missingness visualizations\n",
      "2025-12-09 21:33:28 - eda_pipeline - WARNING - missingno failed, using bar fallback: name 'msno' is not defined\n",
      "2025-12-09 21:33:29 - eda_pipeline - INFO - ======================================================================\n",
      "2025-12-09 21:33:29 - eda_pipeline - INFO - Minimal EDA Pipeline Complete ✅\n",
      "2025-12-09 21:33:29 - eda_pipeline - INFO - Plots saved to: notebooks/plots\n",
      "2025-12-09 21:33:29 - eda_pipeline - INFO - ======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXECUTION\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    run_eda_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
